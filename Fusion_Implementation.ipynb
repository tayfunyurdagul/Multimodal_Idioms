{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "QqmBtAdes6T8"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torchvision pandas pillow tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 1. SETUP\n",
        "# ==========================================\n",
        "!pip install -q transformers torchvision pandas pillow tqdm\n",
        "\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import models\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab import drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Connect to your shared modular file\n",
        "sys.path.append('/content/drive/My Drive/AdMIRe_Project/')\n",
        "import admire_dataset\n",
        "import importlib\n",
        "importlib.reload(admire_dataset) # Reload to be safe\n",
        "from admire_dataset import AdMIReReader\n",
        "\n",
        "# ==========================================\n",
        "# 2. LOAD DATASET\n",
        "# ==========================================\n",
        "EXTRACT_PATH = \"/content/admire_data\"\n",
        "\n",
        "print(\"ðŸ“‚ Initializing Fusion Reader...\")\n",
        "try:\n",
        "    # mode='fusion' tells the reader to return PIXEL TENSORS\n",
        "    # The reader will automatically find 'subtask_a_train.tsv' in the subfolder\n",
        "    train_dataset = AdMIReReader(data_root_path=EXTRACT_PATH, split=\"Train\", mode=\"fusion\")\n",
        "    print(f\"âœ… Fusion Dataset Loaded: {len(train_dataset)} items\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error: {e}\")\n",
        "    raise"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-kkrW1YxtPb_",
        "outputId": "6e6aed2c-8c07-4642-dd07-6eb8698a98aa"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“‚ Initializing Fusion Reader...\n",
            "ðŸ•µï¸ Scanning /content/admire_data for Train data...\n",
            "âœ… Loaded: /content/admire_data/train/subtask_a_train.tsv\n",
            "âœ… Fusion Dataset Loaded: 70 items\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ==========================================\n",
        "# 3. DEFINE MODEL (ResNet50 + DistilBERT)\n",
        "# ==========================================\n",
        "class IdiomFusionNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(IdiomFusionNet, self).__init__()\n",
        "\n",
        "        # --- TOWER 1: VISION (ResNet50) ---\n",
        "        # Load pre-trained ResNet and remove the classification head\n",
        "        resnet = models.resnet50(pretrained=True)\n",
        "        self.vision_encoder = nn.Sequential(*list(resnet.children())[:-1]) # Output: 2048 dims\n",
        "        self.vision_proj = nn.Linear(2048, 256) # Project to 256 dims\n",
        "\n",
        "        # --- TOWER 2: TEXT (DistilBERT) ---\n",
        "        self.text_model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.text_proj = nn.Linear(768, 256) # Project to 256 dims\n",
        "\n",
        "    def forward(self, images, input_ids, attention_mask):\n",
        "        \"\"\"\n",
        "        images: [Batch, 5, 3, 224, 224]\n",
        "        input_ids: Text tokens\n",
        "        \"\"\"\n",
        "        batch_size = images.shape[0]\n",
        "\n",
        "        # A. ENCODE TEXT\n",
        "        text_out = self.text_model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        text_emb = text_out.last_hidden_state[:, 0, :] # [CLS] token\n",
        "        text_emb = self.text_proj(text_emb) # [Batch, 256]\n",
        "\n",
        "        # B. ENCODE IMAGES\n",
        "        # Flatten batch to process 5 images at once\n",
        "        images_flat = images.view(-1, 3, 224, 224)\n",
        "\n",
        "        img_feats = self.vision_encoder(images_flat).squeeze() # [Batch*5, 2048]\n",
        "        img_emb = self.vision_proj(img_feats) # [Batch*5, 256]\n",
        "\n",
        "        # Reshape back to [Batch, 5, 256]\n",
        "        img_emb = img_emb.view(batch_size, 5, 256)\n",
        "\n",
        "        # C. FUSION (Dot Product)\n",
        "        text_emb_expanded = text_emb.unsqueeze(1) # [Batch, 1, 256]\n",
        "        scores = torch.sum(text_emb_expanded * img_emb, dim=2) # [Batch, 5]\n",
        "\n",
        "        return scores"
      ],
      "metadata": {
        "id": "sYee0eKWtRME"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# ==========================================\n",
        "# 4. TRAINING CONFIGURATION\n",
        "# ==========================================\n",
        "# We initialize the tokenizer here for the collate function\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def fusion_collate(batch):\n",
        "    images = torch.stack([item['pixel_values'] for item in batch])\n",
        "    labels = torch.stack([item['labels'] for item in batch])\n",
        "    texts = [item['raw_text'] for item in batch]\n",
        "\n",
        "    # Tokenize text\n",
        "    toks = tokenizer(texts, padding=True, truncation=True, max_length=64, return_tensors=\"pt\")\n",
        "\n",
        "    # Return texts too so we can use them in the final report\n",
        "    return images, toks['input_ids'], toks['attention_mask'], labels, texts\n",
        "\n",
        "# DataLoader (Batch Size 16 is safe for T4 GPU)\n",
        "BATCH_SIZE = 16\n",
        "loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=fusion_collate)\n",
        "\n",
        "# Setup Device & Optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = IdiomFusionNet().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# ==========================================\n",
        "# 5. TRAINING LOOP\n",
        "# ==========================================\n",
        "EPOCHS = 5\n",
        "print(f\"\\nðŸš€ Starting Full Training on {device}...\")\n",
        "print(f\"âš¡ Configuration: {len(train_dataset)} items | Batch Size {BATCH_SIZE} | {EPOCHS} Epochs\")\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    progress = tqdm(loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "    for imgs, input_ids, mask, labels, _ in progress:\n",
        "        imgs, input_ids, mask, labels = imgs.to(device), input_ids.to(device), mask.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward Pass\n",
        "        logits = model(imgs, input_ids, mask)\n",
        "\n",
        "        # Loss Calculation\n",
        "        loss = criterion(logits, labels)\n",
        "\n",
        "        # Backward Pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Metrics\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Live Progress Bar Update\n",
        "        progress.set_postfix({\"Loss\": loss.item(), \"Acc\": correct/total})\n",
        "\n",
        "    # Epoch Summary\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    final_acc = correct / total\n",
        "    print(f\"âœ… Epoch {epoch+1} Done. Avg Loss: {avg_loss:.4f} | Accuracy: {final_acc:.4f}\")\n",
        "\n",
        "# Save the Model\n",
        "save_path = \"/content/drive/My Drive/AdMIRe_Project/fusion_model_full.pth\"\n",
        "torch.save(model.state_dict(), save_path)\n",
        "print(f\"ðŸ’¾ Trained Model Saved to: {save_path}\")\n",
        "\n",
        "# ==========================================\n",
        "# 6. GENERATE READABLE REPORT\n",
        "# ==========================================\n",
        "print(\"\\nðŸ“ Generating Full Report (Evaluation Pass)...\")\n",
        "model.eval()\n",
        "results = []\n",
        "\n",
        "# Create a non-shuffled loader for the final report\n",
        "report_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=fusion_collate)\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, input_ids, mask, labels, raw_texts in tqdm(report_loader, desc=\"Evaluating\"):\n",
        "        imgs, input_ids, mask, labels = imgs.to(device), input_ids.to(device), mask.to(device), labels.to(device)\n",
        "\n",
        "        # Get Scores\n",
        "        logits = model(imgs, input_ids, mask)\n",
        "        probs = F.softmax(logits, dim=1) # Convert to % confidence\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "        for i in range(len(labels)):\n",
        "            truth = labels[i].item()\n",
        "            prediction = preds[i].item()\n",
        "            confidence = probs[i][prediction].item() * 100\n",
        "\n",
        "            results.append({\n",
        "                \"Idiom_Sentence\": raw_texts[i],\n",
        "                \"True_Image_No\": truth + 1,       # Convert index 0 -> Image 1\n",
        "                \"Predicted_Image_No\": prediction + 1,\n",
        "                \"Result\": \"CORRECT\" if truth == prediction else \"WRONG\",\n",
        "                \"Confidence\": f\"{confidence:.1f}%\"\n",
        "            })\n",
        "\n",
        "# Save to Drive\n",
        "report_file = \"/content/drive/My Drive/AdMIRe_Project/fusion_full_report.tsv\"\n",
        "df = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ðŸ“Š FINAL REPORT SUMMARY\")\n",
        "print(f\"Total Items: {len(df)}\")\n",
        "print(f\"Overall Accuracy: {(df['Result']=='CORRECT').mean()*100:.2f}%\")\n",
        "print(\"-\" * 60)\n",
        "print(\"Preview (First 10 Rows):\")\n",
        "print(df[['True_Image_No', 'Predicted_Image_No', 'Result', 'Confidence']].head(10))\n",
        "print(\"=\"*60)\n",
        "\n",
        "df.to_csv(report_file, sep='\\t', index=False)\n",
        "print(f\"ðŸ’¾ Full readable report saved to: {report_file}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g7smonYMvGM",
        "outputId": "2684dce0-9339-4010-a5d5-bd6a3e96f7cf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ðŸš€ Starting Full Training on cuda...\n",
            "âš¡ Configuration: 70 items | Batch Size 16 | 5 Epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:17<00:00,  3.56s/it, Loss=1.46, Acc=0.229]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 1 Done. Avg Loss: 1.8498 | Accuracy: 0.2286\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:14<00:00,  2.95s/it, Loss=0.0454, Acc=0.8]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 2 Done. Avg Loss: 0.4197 | Accuracy: 0.8000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:14<00:00,  2.96s/it, Loss=0.00162, Acc=0.943]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 3 Done. Avg Loss: 0.0987 | Accuracy: 0.9429\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:15<00:00,  3.10s/it, Loss=0.000222, Acc=0.957]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 4 Done. Avg Loss: 0.0708 | Accuracy: 0.9571\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/5: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:15<00:00,  3.02s/it, Loss=7.95e-5, Acc=0.957]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Epoch 5 Done. Avg Loss: 0.0691 | Accuracy: 0.9571\n",
            "ðŸ’¾ Trained Model Saved to: /content/drive/My Drive/AdMIRe_Project/fusion_model_full.pth\n",
            "\n",
            "ðŸ“ Generating Full Report (Evaluation Pass)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:20<00:00,  4.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ðŸ“Š FINAL REPORT SUMMARY\n",
            "Total Items: 70\n",
            "Overall Accuracy: 95.71%\n",
            "------------------------------------------------------------\n",
            "Preview (First 10 Rows):\n",
            "   True_Image_No  Predicted_Image_No   Result Confidence\n",
            "0              1                   1  CORRECT     100.0%\n",
            "1              2                   2  CORRECT     100.0%\n",
            "2              3                   3  CORRECT     100.0%\n",
            "3              4                   4  CORRECT     100.0%\n",
            "4              4                   4  CORRECT     100.0%\n",
            "5              2                   2  CORRECT     100.0%\n",
            "6              5                   5  CORRECT     100.0%\n",
            "7              1                   1  CORRECT     100.0%\n",
            "8              2                   1    WRONG      20.0%\n",
            "9              4                   4  CORRECT     100.0%\n",
            "============================================================\n",
            "ðŸ’¾ Full readable report saved to: /content/drive/My Drive/AdMIRe_Project/fusion_full_report.tsv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ]
}